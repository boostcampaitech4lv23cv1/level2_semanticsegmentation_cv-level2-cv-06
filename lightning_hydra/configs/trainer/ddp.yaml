defaults:
  - default.yaml

# use "ddp_spawn" instead of "ddp",
# it's slower but normal "ddp" currently doesn't work ideally with hydra
# https://github.com/facebookresearch/hydra/issues/2070
# https://pytorch-lightning.readthedocs.io/en/latest/accelerators/gpu_intermediate.html#distributed-data-parallel-spawn
strategy: ddp_spawn_find_unused_parameters_false

accelerator: gpu
devices: [0, 1, 2, 3]
num_nodes: 1
sync_batchnorm: True
# plugins:
#  _target_: pytorch_lightning.plugins.DDPPlugin
#  find_unused_parameters: False
